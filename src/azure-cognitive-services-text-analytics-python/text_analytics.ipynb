{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "51c3f01bf447baff25274d03ed6de26a7d16ffbc58ea7558f7faace83c35fc62"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "'C:\\Users\\David' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYTICS_KEY = \"<your analytics key>\"\n",
    "ANALYTICS_ENDPOINT = \"<your analytics endpoint>\""
   ]
  },
  {
   "source": [
    "## Authenticate the client"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(ANALYTICS_KEY)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=ANALYTICS_ENDPOINT, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()"
   ]
  },
  {
   "source": [
    "## Sentiment analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Document: I had the best day of my life. I wish you were there with me.\n",
      "\n",
      "Document Sentiment: positive\n",
      "Overall scores: positive=1.00; neutral=0.00; negative=0.00 \n",
      "\n",
      "Sentence: I had the best day of my life.\n",
      "Sentence 1 sentiment: positive\n",
      "Sentence score:\n",
      "Positive=1.00\n",
      "Neutral=0.00\n",
      "Negative=0.00\n",
      "\n",
      "Sentence: I wish you were there with me.\n",
      "Sentence 2 sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.21\n",
      "Neutral=0.77\n",
      "Negative=0.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sentiment_analysis_example(client):\n",
    "\n",
    "    documents = [\"I had the best day of my life. I wish you were there with me.\"]\n",
    "    print(\"Document: {}\\n\".format(documents[0]))\n",
    "    response = client.analyze_sentiment(documents=documents)[0]\n",
    "    print(\"Document Sentiment: {}\".format(response.sentiment))\n",
    "    print(\"Overall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
    "        response.confidence_scores.positive,\n",
    "        response.confidence_scores.neutral,\n",
    "        response.confidence_scores.negative,\n",
    "    ))\n",
    "    for idx, sentence in enumerate(response.sentences):\n",
    "        print(\"Sentence: {}\".format(sentence.text))\n",
    "        print(\"Sentence {} sentiment: {}\".format(idx+1, sentence.sentiment))\n",
    "        print(\"Sentence score:\\nPositive={0:.2f}\\nNeutral={1:.2f}\\nNegative={2:.2f}\\n\".format(\n",
    "            sentence.confidence_scores.positive,\n",
    "            sentence.confidence_scores.neutral,\n",
    "            sentence.confidence_scores.negative,\n",
    "        ))\n",
    "          \n",
    "sentiment_analysis_example(client)"
   ]
  },
  {
   "source": [
    "## Opinion mining"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Document: The food and service were unacceptable, but the concierge were nice\n",
      "\n",
      "Document Sentiment: positive\n",
      "Overall scores: positive=0.84; neutral=0.00; negative=0.16 \n",
      "\n",
      "Sentence: The food and service were unacceptable, but the concierge were nice\n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=0.84\n",
      "Neutral=0.00\n",
      "Negative=0.16\n",
      "\n",
      "......'negative' aspect 'food'\n",
      "......Aspect score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' opinion 'unacceptable'\n",
      "......Opinion score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' aspect 'service'\n",
      "......Aspect score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' opinion 'unacceptable'\n",
      "......Opinion score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'positive' aspect 'concierge'\n",
      "......Aspect score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' opinion 'nice'\n",
      "......Opinion score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sentiment_analysis_with_opinion_mining_example(client):\n",
    "\n",
    "    documents = [\n",
    "        \"The food and service were unacceptable, but the concierge were nice\"\n",
    "    ]\n",
    "    print(\"Document: {}\\n\".format(documents[0]))\n",
    "\n",
    "    result = client.analyze_sentiment(documents, show_opinion_mining=True)\n",
    "    doc_result = [doc for doc in result if not doc.is_error]\n",
    "\n",
    "    positive_reviews = [doc for doc in doc_result if doc.sentiment == \"positive\"]\n",
    "    negative_reviews = [doc for doc in doc_result if doc.sentiment == \"negative\"]\n",
    "\n",
    "    positive_mined_opinions = []\n",
    "    mixed_mined_opinions = []\n",
    "    negative_mined_opinions = []\n",
    "\n",
    "    for document in doc_result:\n",
    "        print(\"Document Sentiment: {}\".format(document.sentiment))\n",
    "        print(\"Overall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
    "            document.confidence_scores.positive,\n",
    "            document.confidence_scores.neutral,\n",
    "            document.confidence_scores.negative,\n",
    "        ))\n",
    "        for sentence in document.sentences:\n",
    "            print(\"Sentence: {}\".format(sentence.text))\n",
    "            print(\"Sentence sentiment: {}\".format(sentence.sentiment))\n",
    "            print(\"Sentence score:\\nPositive={0:.2f}\\nNeutral={1:.2f}\\nNegative={2:.2f}\\n\".format(\n",
    "                sentence.confidence_scores.positive,\n",
    "                sentence.confidence_scores.neutral,\n",
    "                sentence.confidence_scores.negative,\n",
    "            ))\n",
    "            for mined_opinion in sentence.mined_opinions:\n",
    "                aspect = mined_opinion.aspect\n",
    "                print(\"......'{}' aspect '{}'\".format(aspect.sentiment, aspect.text))\n",
    "                print(\"......Aspect score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
    "                    aspect.confidence_scores.positive,\n",
    "                    aspect.confidence_scores.negative,\n",
    "                ))\n",
    "                for opinion in mined_opinion.opinions:\n",
    "                    print(\"......'{}' opinion '{}'\".format(opinion.sentiment, opinion.text))\n",
    "                    print(\"......Opinion score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
    "                        opinion.confidence_scores.positive,\n",
    "                        opinion.confidence_scores.negative,\n",
    "                    ))\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "          \n",
    "sentiment_analysis_with_opinion_mining_example(client)"
   ]
  },
  {
   "source": [
    "## Language detection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Document: Ce document est rédigé en Français.\n",
      "\n",
      "Language:  French\n"
     ]
    }
   ],
   "source": [
    "def language_detection_example(client):\n",
    "    try:\n",
    "        documents = [\"Ce document est rédigé en Français.\"]\n",
    "        print(\"Document: {}\\n\".format(documents[0]))\n",
    "\n",
    "        response = client.detect_language(documents, country_hint = '')[0]\n",
    "        print(\"Language: \", response.primary_language.name)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "\n",
    "language_detection_example(client)"
   ]
  },
  {
   "source": [
    "## Named Entity recognition (NER)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Document: I had a wonderful trip to Seattle last week.\n\nNamed Entities:\n\n\tText: \t trip \tCategory: \t Event \tSubCategory: \t None \n\tConfidence Score: \t 0.61 \n\tOffset: \t 18 \n\n\tText: \t Seattle \tCategory: \t Location \tSubCategory: \t GPE \n\tConfidence Score: \t 0.82 \n\tOffset: \t 26 \n\n\tText: \t last week \tCategory: \t DateTime \tSubCategory: \t DateRange \n\tConfidence Score: \t 0.8 \n\tOffset: \t 34 \n\n"
     ]
    }
   ],
   "source": [
    "def entity_recognition_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\"I had a wonderful trip to Seattle last week.\"]\n",
    "        print(\"Document: {}\\n\".format(documents[0]))\n",
    "        \n",
    "        result = client.recognize_entities(documents = documents)[0]\n",
    "\n",
    "        print(\"Named Entities:\\n\")\n",
    "        for entity in result.entities:\n",
    "            print(\"\\tText: \\t\", entity.text, \"\\tCategory: \\t\", entity.category, \"\\tSubCategory: \\t\", entity.subcategory,\n",
    "                    \"\\n\\tConfidence Score: \\t\", round(entity.confidence_score, 2), \"\\n\\tOffset: \\t\", entity.offset, \"\\n\")\n",
    "            if hasattr(entity, 'length'):\n",
    "                print(\"\\n\\tLength: \\t\", entity.length)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "\n",
    "entity_recognition_example(client)"
   ]
  },
  {
   "source": [
    "## Entity Linking"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Document: Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975, \n        to develop and sell BASIC interpreters for the Altair 8800. \n        During his career at Microsoft, Gates held the positions of chairman,\n        chief executive officer, president and chief software architect, \n        while also being the largest individual shareholder until May 2014.\n\nLinked Entities:\n\n\tName:  Microsoft \tId:  Microsoft \tUrl:  https://en.wikipedia.org/wiki/Microsoft \n\tData Source:  Wikipedia\n\tMatches:\n\t\tText: Microsoft\n\t\tConfidence Score: 0.55\n\t\tOffset: 0\n\t\tText: Microsoft\n\t\tConfidence Score: 0.55\n\t\tOffset: 168\n\tName:  Bill Gates \tId:  Bill Gates \tUrl:  https://en.wikipedia.org/wiki/Bill_Gates \n\tData Source:  Wikipedia\n\tMatches:\n\t\tText: Bill Gates\n\t\tConfidence Score: 0.63\n\t\tOffset: 25\n\t\tText: Gates\n\t\tConfidence Score: 0.63\n\t\tOffset: 179\n\tName:  Paul Allen \tId:  Paul Allen \tUrl:  https://en.wikipedia.org/wiki/Paul_Allen \n\tData Source:  Wikipedia\n\tMatches:\n\t\tText: Paul Allen\n\t\tConfidence Score: 0.60\n\t\tOffset: 40\n\tName:  April 4 \tId:  April 4 \tUrl:  https://en.wikipedia.org/wiki/April_4 \n\tData Source:  Wikipedia\n\tMatches:\n\t\tText: April 4\n\t\tConfidence Score: 0.32\n\t\tOffset: 54\n\tName:  BASIC \tId:  BASIC \tUrl:  https://en.wikipedia.org/wiki/BASIC \n\tData Source:  Wikipedia\n\tMatches:\n\t\tText: BASIC\n\t\tConfidence Score: 0.33\n\t\tOffset: 98\n\tName:  Altair 8800 \tId:  Altair 8800 \tUrl:  https://en.wikipedia.org/wiki/Altair_8800 \n\tData Source:  Wikipedia\n\tMatches:\n\t\tText: Altair 8800\n\t\tConfidence Score: 0.88\n\t\tOffset: 125\n"
     ]
    }
   ],
   "source": [
    "def entity_linking_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\"\"\"Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975, \n",
    "        to develop and sell BASIC interpreters for the Altair 8800. \n",
    "        During his career at Microsoft, Gates held the positions of chairman,\n",
    "        chief executive officer, president and chief software architect, \n",
    "        while also being the largest individual shareholder until May 2014.\"\"\"]\n",
    "        print(\"Document: {}\\n\".format(documents[0]))\n",
    "\n",
    "        result = client.recognize_linked_entities(documents = documents)[0]\n",
    "\n",
    "        print(\"Linked Entities:\\n\")\n",
    "        for entity in result.entities:\n",
    "            print(\"\\tName: \", entity.name, \"\\tId: \", entity.data_source_entity_id, \"\\tUrl: \", entity.url,\n",
    "            \"\\n\\tData Source: \", entity.data_source)\n",
    "            print(\"\\tMatches:\")\n",
    "            for match in entity.matches:\n",
    "                print(\"\\t\\tText:\", match.text)\n",
    "                print(\"\\t\\tConfidence Score: {0:.2f}\".format(match.confidence_score))\n",
    "                if hasattr(match, 'offset'):\n",
    "                    print(\"\\t\\tOffset: {}\".format(match.offset))\n",
    "                if hasattr(match, 'length'):\n",
    "                    print(\"\\t\\tLength: {}\".format(match.length))\n",
    "            \n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "\n",
    "entity_linking_example(client)"
   ]
  },
  {
   "source": [
    "## Personally Identifiable Information recognition (PII)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Document 1: The employee's SSN is 859-98-0987.\n",
      "\n",
      "Document 2: The employee's phone number is 555-555-5555.\n",
      "\n",
      "\n",
      "Redacted Text: The employee's SSN is ***********.\n",
      "Entity: 859-98-0987\n",
      "\tCategory: U.S. Social Security Number (SSN)\n",
      "\tConfidence Score: 0.65\n",
      "\tOffset: 22\n",
      "\n",
      "Redacted Text: The employee's phone number is ************.\n",
      "Entity: 555-555-5555\n",
      "\tCategory: Phone Number\n",
      "\tConfidence Score: 0.8\n",
      "\tOffset: 31\n"
     ]
    }
   ],
   "source": [
    "def pii_recognition_example(client):\n",
    "    documents = [\n",
    "        \"The employee's SSN is 859-98-0987.\",\n",
    "        \"The employee's phone number is 555-555-5555.\"\n",
    "    ]\n",
    "    print(\"Document 1: {}\\n\".format(documents[0]))\n",
    "    print(\"Document 2: {}\\n\".format(documents[1]))\n",
    "\n",
    "    response = client.recognize_pii_entities(documents, language=\"en\")\n",
    "    result = [doc for doc in response if not doc.is_error]\n",
    "    for doc in result:\n",
    "        print(\"\\nRedacted Text: {}\".format(doc.redacted_text))\n",
    "        for entity in doc.entities:\n",
    "            print(\"Entity: {}\".format(entity.text))\n",
    "            print(\"\\tCategory: {}\".format(entity.category))\n",
    "            print(\"\\tConfidence Score: {}\".format(entity.confidence_score))\n",
    "            if hasattr(entity, 'offset'):\n",
    "                print(\"\\tOffset: {}\".format(entity.offset))\n",
    "            if hasattr(entity, 'length'):\n",
    "                print(\"\\tLength: {}\".format(entity.length))\n",
    "\n",
    "pii_recognition_example(client)"
   ]
  },
  {
   "source": [
    "## Key phrase extraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Document: My cat might need to see a veterinarian.\n\n\tKey Phrases:\n\t\t cat\n\t\t veterinarian\n"
     ]
    }
   ],
   "source": [
    "def key_phrase_extraction_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\"My cat might need to see a veterinarian.\"]\n",
    "        print(\"Document: {}\\n\".format(documents[0]))\n",
    "\n",
    "        response = client.extract_key_phrases(documents = documents)[0]\n",
    "\n",
    "        if not response.is_error:\n",
    "            print(\"\\tKey Phrases:\")\n",
    "            for phrase in response.key_phrases:\n",
    "                print(\"\\t\\t\", phrase)\n",
    "        else:\n",
    "            print(response.id, response.error)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "\n",
    "key_phrase_extraction_example(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}